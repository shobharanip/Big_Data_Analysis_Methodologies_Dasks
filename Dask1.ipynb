{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tu5aXSLcfdEo",
        "outputId": "1679d060-3771-4dd6-8fb8-fe1aa5869b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Using cached pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=14fcd0cb97461b2e8c2fb7a85602dae585644156d5288a6c25c907e918af24c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4f1UaRLd7iH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad6d7a5-2b70-4974-da56-60a470e016e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "collection = [1,\"two\",3.0, (\"four\",4),{\"five\":5}]\n",
        "sc = spark.sparkContext\n",
        "collection_rdd= sc.parallelize(collection)\n",
        "print(collection_rdd)\n",
        "#ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from py4j.protocol import Py4JJavaError\n",
        "\n",
        "def add_one(value):\n",
        "  return value + 1\n",
        "\n",
        "\n",
        "collection_rdd = collection_rdd.map(add_one)\n",
        "\n",
        "try:\n",
        "  print (collection_rdd.collect())\n",
        "except Py4JJavaError:\n",
        "  pass\n",
        "\n",
        "print(add_one(8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT_yBTGGgljT",
        "outputId": "f854a195-665f-4b18-9d00-f9d03b685dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collection_rdd = sc.parallelize(collection)\n",
        "\n",
        "def safer_add_one(value):\n",
        "  try:\n",
        "    return value + 1\n",
        "  except TypeError:\n",
        "    return value\n",
        "collection_rdd = collection_rdd.map(safer_add_one)\n",
        "\n",
        "print(collection_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtbltoZuiQRT",
        "outputId": "7c4ed387-3346-44b5-b4f8-083e8443b05c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 'two', 4.0, ('four', 4), {'five': 5}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collection_rdd = collection_rdd.filter(lambda elem: isinstance(elem,(float,int)))\n",
        "\n",
        "print(collection_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmnDpNE3jiSm",
        "outputId": "1003e205-725a-4c3c-8472-16c7b11fe316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "collection_rdd = sc.parallelize([4,7,9,1,3])\n",
        "print(collection_rdd.reduce(add))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaFmpgcEj0XH",
        "outputId": "a1c9e1f7-5d76-4f19-93b0-39641f4c63f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E07GLDtmkeAs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}